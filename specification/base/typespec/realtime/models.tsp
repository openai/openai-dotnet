/*
 * This file was automatically generated from an OpenAPI .yaml file.
 * Edits made directly to this file will be lost.
 */

import "../common";
import "./custom.tsp";

using Http;
using TypeSpec.OpenAPI;

namespace OpenAI;

// Tool generated type. Extracts from RealtimeConversationItemWithReference.content
alias RealtimeConversationItemWithReferenceContent = {
  @doc("""
    The content type (`input_text`, `input_audio`, `item_reference`, `text`).
    """)
  type?: "input_audio" | "input_text" | "item_reference" | "text";

  @doc("""
    The text content, used for `input_text` and `text` content types.
    """)
  text?: string;

  @doc("""
    ID of a previous conversation item to reference (for `item_reference`
    content types in `response.create` events). These can reference both
    client and server created items.
    """)
  id?: string;

  @doc("""
    Base64-encoded audio bytes, used for `input_audio` content type.
    """)
  audio?: string;

  @doc("""
    The transcript of the audio, used for `input_audio` content type.
    """)
  transcript?: string;
};

// Tool customization: Adjust union to be a discriminated type base
/** A realtime client event. */
@discriminator("type")
model RealtimeClientEvent {
  /** The type of event. */
  type: RealtimeClientEventType;

  event_id?: string;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event to update the session’s default configuration.
  The client may send this event at any time to update any field,
  except for `voice`. However, note that once a session has been
  initialized with a particular `model`, it can’t be changed to
  another model using `session.update`.
  
  When the server receives a `session.update`, it will respond
  with a `session.updated` event showing the full, effective configuration.
  Only the fields that are present are updated. To clear a field like
  `instructions`, pass an empty string.
  """)
model RealtimeClientEventSessionUpdate extends RealtimeClientEvent {
  @doc("""
    The event type, must be `session.update`.
    """)
  type: RealtimeClientEventType.session_update;

  /**
   * Update the Realtime session. Choose either a realtime session or a transcription session.
   */
  session: RealtimeRequestSessionBase;
}

// Tool customization: establish custom, enriched discriminated type hierarchy
/** The item to add to the conversation. */
model RealtimeConversationItemBase {
  /** Customized to enriched RealtimeConversation{Request,Response}Item models */
}

/** The response resource. */
model RealtimeResponse {
  /** The unique ID of the response. */
  id?: string;

  @doc("""
    The object type, must be `realtime.response`.
    """)
  object?: "realtime.response";

  @doc("""
    The final status of the response (`completed`, `cancelled`, `failed`,
    `incomplete`, or `in_progress`).
    """)
  status?: "completed" | "cancelled" | "failed" | "incomplete" | "in_progress";

  /** Additional details about the status. */
  status_details?: {
    @doc("""
      The type of error that caused the response to fail, corresponding
      with the `status` field (`completed`, `cancelled`, `incomplete`,
      `failed`).
      """)
    type?: "completed" | "cancelled" | "failed" | "incomplete";

    @doc("""
      The reason the Response did not complete. For a `cancelled` Response,
      one of `turn_detected` (the server VAD detected a new start of speech)
      or `client_cancelled` (the client sent a cancel event). For an
      `incomplete` Response, one of `max_output_tokens` or `content_filter`
      (the server-side safety filter activated and cut off the response).
      """)
    reason?:
      | "turn_detected"
      | "client_cancelled"
      | "max_output_tokens"
      | "content_filter";

    @doc("""
      A description of the error that caused the response to fail,
      populated when the `status` is `failed`.
      """)
    error?: {
      /** The type of error. */
      type?: string;

      /** Error code, if any. */
      code?: string;
    };
  };

  // Tool customization: apply enriched response-specific type
  /** The list of output items generated by the response. */
  output?: RealtimeConversationResponseItem[];

  ...MetadataPropertyForResponse;

  /**
   * Usage statistics for the Response, this will correspond to billing. A
   * Realtime API session will maintain a conversation context and append new
   * Items to the Conversation, thus output from previous turns (text and
   * audio tokens) will become the input for later turns.
   */
  usage?: {
    /**
     * The total number of tokens in the Response including input and output
     * text and audio tokens.
     */
    total_tokens?: int32;

    /**
     * The number of input tokens used in the Response, including text and
     * audio tokens.
     */
    input_tokens?: int32;

    /**
     * The number of output tokens sent in the Response, including text and
     * audio tokens.
     */
    output_tokens?: int32;

    /** Details about the input tokens used in the Response. */
    input_token_details?: {
      /** The number of cached tokens used in the Response. */
      cached_tokens?: int32;

      /** The number of text tokens used in the Response. */
      text_tokens?: int32;

      /** The number of audio tokens used in the Response. */
      audio_tokens?: int32;
    };

    /** Details about the output tokens used in the Response. */
    output_token_details?: {
      /** The number of text tokens used in the Response. */
      text_tokens?: int32;

      /** The number of audio tokens used in the Response. */
      audio_tokens?: int32;
    };
  };

  @doc("""
    Which conversation the response is added to, determined by the `conversation`
    field in the `response.create` event. If `auto`, the response will be added to
    the default conversation and the value of `conversation_id` will be an id like
    `conv_1234`. If `none`, the response will not be added to any conversation and
    the value of `conversation_id` will be `null`. If responses are being triggered
    by server VAD, the response will be added to the default conversation, thus
    the `conversation_id` will be an id like `conv_1234`.
    """)
  conversation_id?: string;

  @doc("""
    The voice the model used to respond.
    Current voice options are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`,
    `onyx`, `nova`, `sage`, `shimmer`, and `verse`.
    """)
  voice?: VoiceIdsShared;

  @doc("""
    The set of modalities the model used to respond. If there are multiple modalities,
    the model will pick one, for example if `modalities` is `["text", "audio"]`, the model
    could be responding in either text or audio.
    """)
  modalities?: ("text" | "audio")[];

  @doc("""
    The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    """)
  output_audio_format?: "pcm16" | "g711_ulaw" | "g711_alaw";

  /** Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. */
  temperature?: float32;

  /**
   * Maximum number of output tokens for a single assistant response,
   * inclusive of tool calls, that was used in this response.
   */
  max_output_tokens?: int32 | "inf";
}

// Tool customization (apply_discriminator): apply discriminated type base
/**
 * Send this event to append audio bytes to the input audio buffer. The audio
 * buffer is temporary storage you can write to and later commit. In Server VAD
 * mode, the audio buffer is used to detect speech and the server will decide
 * when to commit. When Server VAD is disabled, you must commit the audio buffer
 * manually.
 *
 * The client may choose how much audio to place in each event up to a maximum
 * of 15 MiB, for example streaming smaller chunks from the client may allow the
 * VAD to be more responsive. Unlike made other client events, the server will
 * not send a confirmation response to this event.
 */
model RealtimeClientEventInputAudioBufferAppend extends RealtimeClientEvent {
  @doc("""
    The event type, must be `input_audio_buffer.append`.
    """)
  type: RealtimeClientEventType.input_audio_buffer_append;

  // Tool customization: use encoded type for audio data
  @doc("""
    Base64-encoded audio bytes. This must be in the format specified by the
    `input_audio_format` field in the session configuration.
    """)
  @encode("base64")
  audio: bytes;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event to commit the user input audio buffer, which will create a
  new user message item in the conversation. This event will produce an error
  if the input audio buffer is empty. When in Server VAD mode, the client does
  not need to send this event, the server will commit the audio buffer
  automatically.
  
  Committing the input audio buffer will trigger input audio transcription
  (if enabled in session configuration), but it will not create a response
  from the model. The server will respond with an `input_audio_buffer.committed`
  event.
  """)
model RealtimeClientEventInputAudioBufferCommit extends RealtimeClientEvent {
  @doc("""
    The event type, must be `input_audio_buffer.commit`.
    """)
  type: RealtimeClientEventType.input_audio_buffer_commit;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event to clear the audio bytes in the buffer. The server will
  respond with an `input_audio_buffer.cleared` event.
  """)
model RealtimeClientEventInputAudioBufferClear extends RealtimeClientEvent {
  @doc("""
    The event type, must be `input_audio_buffer.clear`.
    """)
  type: RealtimeClientEventType.input_audio_buffer_clear;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  **WebRTC Only:** Emit to cut off the current audio response. This will trigger the server to
  stop generating audio and emit a `output_audio_buffer.cleared` event. This
  event should be preceded by a `response.cancel` client event to stop the
  generation of the current response.
  [Learn more](/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).
  """)
model RealtimeClientEventOutputAudioBufferClear extends RealtimeClientEvent {
  @doc("""
    The event type, must be `output_audio_buffer.clear`.
    """)
  type: RealtimeClientEventType.output_audio_buffer_clear;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Add a new Item to the Conversation's context, including messages, function
  calls, and function call responses. This event can be used both to populate a
  "history" of the conversation and to add new items mid-stream, but has the
  current limitation that it cannot populate assistant audio messages.
  
  If successful, the server will respond with a `conversation.item.created`
  event, otherwise an `error` event will be sent.
  """)
model RealtimeClientEventConversationItemCreate extends RealtimeClientEvent {
  @doc("""
    The event type, must be `conversation.item.create`.
    """)
  type: RealtimeClientEventType.conversation_item_create;

  @doc("""
    The ID of the preceding item after which the new item will be inserted.
    If not set, the new item will be appended to the end of the conversation.
    If set to `root`, the new item will be added to the beginning of the conversation.
    If set to an existing ID, it allows an item to be inserted mid-conversation. If the
    ID cannot be found, an error will be returned and the item will not be added.
    """)
  previous_item_id?: string;

  // Tool customization: apply enriched item definition hierarchy
  item: RealtimeConversationRequestItem;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event to truncate a previous assistant message’s audio. The server
  will produce audio faster than realtime, so this event is useful when the user
  interrupts to truncate audio that has already been sent to the client but not
  yet played. This will synchronize the server's understanding of the audio with
  the client's playback.
  
  Truncating audio will delete the server-side text transcript to ensure there
  is not text in the context that hasn't been heard by the user.
  
  If successful, the server will respond with a `conversation.item.truncated`
  event.
  """)
model RealtimeClientEventConversationItemTruncate extends RealtimeClientEvent {
  @doc("""
    The event type, must be `conversation.item.truncate`.
    """)
  type: RealtimeClientEventType.conversation_item_truncate;

  /**
   * The ID of the assistant message item to truncate. Only assistant message
   * items can be truncated.
   */
  item_id: string;

  /** The index of the content part to truncate. Set this to 0. */
  content_index: int32;

  /**
   * Inclusive duration up to which audio is truncated, in milliseconds. If
   * the audio_end_ms is greater than the actual audio duration, the server
   * will respond with an error.
   */
  audio_end_ms: int32;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event when you want to remove any item from the conversation
  history. The server will respond with a `conversation.item.deleted` event,
  unless the item does not exist in the conversation history, in which case the
  server will respond with an error.
  """)
model RealtimeClientEventConversationItemDelete extends RealtimeClientEvent {
  @doc("""
    The event type, must be `conversation.item.delete`.
    """)
  type: RealtimeClientEventType.conversation_item_delete;

  /** The ID of the item to delete. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  This event instructs the server to create a Response, which means triggering
  model inference. When in Server VAD mode, the server will create Responses
  automatically.
  
  A Response will include at least one Item, and may have two, in which case
  the second will be a function call. These Items will be appended to the
  conversation history.
  
  The server will respond with a `response.created` event, events for Items
  and content created, and finally a `response.done` event to indicate the
  Response is complete.
  
  The `response.create` event includes inference configuration like
  `instructions`, and `temperature`. These fields will override the Session's
  configuration for this Response only.
  """)
model RealtimeClientEventResponseCreate extends RealtimeClientEvent {
  @doc("""
    The event type, must be `response.create`.
    """)
  type: RealtimeClientEventType.response_create;

  response?: RealtimeResponseCreateParams;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event to cancel an in-progress response. The server will respond
  with a `response.cancelled` event or an error if there is no response to
  cancel.
  """)
model RealtimeClientEventResponseCancel extends RealtimeClientEvent {
  @doc("""
    The event type, must be `response.cancel`.
    """)
  type: RealtimeClientEventType.response_cancel;

  /**
   * A specific response ID to cancel - if not provided, will cancel an
   * in-progress response in the default conversation.
   */
  response_id?: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when an error occurs, which could be a client problem or a server
 * problem. Most errors are recoverable and the session will stay open, we
 * recommend to implementors to monitor and log error messages by default.
 */
model RealtimeServerEventError extends RealtimeServerEvent {
  @doc("""
    The event type, must be `error`.
    """)
  type: RealtimeServerEventType.error;

  /** Details of the error. */
  error: {
    /** The type of error (e.g., "invalid_request_error", "server_error"). */
    type: string;

    /** Error code, if any. */
    code?: string | null;

    /** A human-readable error message. */
    message: string;

    /** Parameter related to the error, if any. */
    param?: string | null;

    /** The event_id of the client event that caused the error, if applicable. */
    event_id?: string | null;
  };
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when a Session is created. Emitted automatically when a new
 * connection is established as the first server event. This event will contain
 * the default Session configuration.
 */
model RealtimeServerEventSessionCreated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `session.created`.
    """)
  type: RealtimeServerEventType.session_created;

  // Tool customization: apply enriched response-specific model
  session: RealtimeResponseSession;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a session is updated with a `session.update` event, unless
  there is an error.
  """)
model RealtimeServerEventSessionUpdated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `session.updated`.
    """)
  type: RealtimeServerEventType.session_updated;

  // Tool customization: apply discriminated session union type
  session: RealtimeSessionUpdatedResponseUnion;
}

// Tool customization: establish base for enriched request/response split models
/** Realtime session object configuration. */
model RealtimeSessionBase {}

// Tool customization: Adjust union to be a discriminated type base
/** A realtime server event. */
@discriminator("type")
model RealtimeServerEvent {
  /** The type of event. */
  type: RealtimeServerEventType;

  event_id?: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when a conversation is created. Emitted right after session creation. */
model RealtimeServerEventConversationCreated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.created`.
    """)
  type: RealtimeServerEventType.conversation_created;

  /** The conversation resource. */
  conversation: {
    /** The unique ID of the conversation. */
    id?: string;

    @doc("""
      The object type, must be `realtime.conversation`.
      """)
    object?: string;
  };
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when an input audio buffer is committed, either by the client or
  automatically in server VAD mode. The `item_id` property is the ID of the user
  message item that will be created, thus a `conversation.item.created` event
  will also be sent to the client.
  """)
model RealtimeServerEventInputAudioBufferCommitted extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.committed`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_committed;

  /** The ID of the preceding item after which the new item will be inserted. */
  previous_item_id: string;

  /** The ID of the user message item that will be created. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when the input audio buffer is cleared by the client with a
  `input_audio_buffer.clear` event.
  """)
model RealtimeServerEventInputAudioBufferCleared extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.cleared`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_cleared;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Sent by the server when in `server_vad` mode to indicate that speech has been
  detected in the audio buffer. This can happen any time audio is added to the
  buffer (unless speech is already detected). The client may want to use this
  event to interrupt audio playback or provide visual feedback to the user.
  
  The client should expect to receive a `input_audio_buffer.speech_stopped` event
  when speech stops. The `item_id` property is the ID of the user message item
  that will be created when speech stops and will also be included in the
  `input_audio_buffer.speech_stopped` event (unless the client manually commits
  the audio buffer during VAD activation).
  """)
model RealtimeServerEventInputAudioBufferSpeechStarted
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.speech_started`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_speech_started;

  @doc("""
    Milliseconds from the start of all audio written to the buffer during the
    session when speech was first detected. This will correspond to the
    beginning of audio sent to the model, and thus includes the
    `prefix_padding_ms` configured in the Session.
    """)
  audio_start_ms: int32;

  /** The ID of the user message item that will be created when speech stops. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned in `server_vad` mode when the server detects the end of speech in
  the audio buffer. The server will also send an `conversation.item.created`
  event with the user message item that is created from the audio buffer.
  """)
model RealtimeServerEventInputAudioBufferSpeechStopped
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.speech_stopped`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_speech_stopped;

  @doc("""
    Milliseconds since the session started when speech stopped. This will
    correspond to the end of audio sent to the model, and thus includes the
    `min_silence_duration_ms` configured in the Session.
    """)
  audio_end_ms: int32;

  /** The ID of the user message item that will be created. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  **WebRTC Only:** Emitted when the output audio buffer is cleared. This happens either in VAD
  mode when the user has interrupted (`input_audio_buffer.speech_started`),
  or when the client has emitted the `output_audio_buffer.clear` event to manually
  cut off the current audio response.
  [Learn more](/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).
  """)
model RealtimeServerEventOutputAudioBufferCleared extends RealtimeServerEvent {
  @doc("""
    The event type, must be `output_audio_buffer.cleared`.
    """)
  type: RealtimeServerEventType.output_audio_buffer_cleared;

  /** The unique ID of the response that produced the audio. */
  response_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  **WebRTC Only:** Emitted when the server begins streaming audio to the client. This event is
  emitted after an audio content part has been added (`response.content_part.added`)
  to the response.
  [Learn more](/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).
  """)
model RealtimeServerEventOutputAudioBufferStarted extends RealtimeServerEvent {
  @doc("""
    The event type, must be `output_audio_buffer.started`.
    """)
  type: RealtimeServerEventType.output_audio_buffer_started;

  /** The unique ID of the response that produced the audio. */
  response_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  **WebRTC Only:** Emitted when the output audio buffer has been completely drained on the server,
  and no more audio is forthcoming. This event is emitted after the full response
  data has been sent to the client (`response.done`).
  [Learn more](/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).
  """)
model RealtimeServerEventOutputAudioBufferStopped extends RealtimeServerEvent {
  @doc("""
    The event type, must be `output_audio_buffer.stopped`.
    """)
  type: RealtimeServerEventType.output_audio_buffer_stopped;

  /** The unique ID of the response that produced the audio. */
  response_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a conversation item is created. There are several scenarios that produce this event:
    - The server is generating a Response, which if successful will produce
      either one or two Items, which will be of type `message`
      (role `assistant`) or type `function_call`.
    - The input audio buffer has been committed, either by the client or the
      server (in `server_vad` mode). The server will take the content of the
      input audio buffer and add it to a new user message Item.
    - The client has sent a `conversation.item.create` event to add a new Item
      to the Conversation.
  """)
model RealtimeServerEventConversationItemCreated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.created`.
    """)
  type: RealtimeServerEventType.conversation_item_created;

  /**
   * The ID of the preceding item in the Conversation context, allows the
   * client to understand the order of the conversation.
   */
  previous_item_id: string;

  // Tool customization: apply enriched item definition hierarchy
  item: RealtimeConversationResponseItem;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  This event is the output of audio transcription for user audio written to the
  user audio buffer. Transcription begins when the input audio buffer is
  committed by the client or server (in `server_vad` mode). Transcription runs
  asynchronously with Response creation, so this event may come before or after
  the Response events.
  
  Realtime API models accept audio natively, and thus input transcription is a
  separate process run on a separate ASR (Automatic Speech Recognition) model,
  currently always `whisper-1`. Thus the transcript may diverge somewhat from
  the model's interpretation, and should be treated as a rough guide.
  """)
model RealtimeServerEventConversationItemInputAudioTranscriptionCompleted
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be
    `conversation.item.input_audio_transcription.completed`.
    """)
  type: RealtimeServerEventType.conversation_item_input_audio_transcription_completed;

  /** The ID of the user message item containing the audio. */
  item_id: string;

  /** The index of the content part containing the audio. */
  content_index: int32;

  /** The transcribed text. */
  transcript: string;

  /** The log probabilities of the transcription. */
  logprobs?: LogProbProperties[] | null;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when input audio transcription is configured, and a transcription
  request for a user message failed. These events are separate from other
  `error` events so that the client can identify the related Item.
  """)
model RealtimeServerEventConversationItemInputAudioTranscriptionFailed
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be
    `conversation.item.input_audio_transcription.failed`.
    """)
  type: RealtimeServerEventType.conversation_item_input_audio_transcription_failed;

  /** The ID of the user message item. */
  item_id: string;

  /** The index of the content part containing the audio. */
  content_index: int32;

  /** Details of the transcription error. */
  error: {
    /** The type of error. */
    type?: string;

    /** Error code, if any. */
    code?: string;

    /** A human-readable error message. */
    message?: string;

    /** Parameter related to the error, if any. */
    param?: string;
  };
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when an earlier assistant audio message item is truncated by the
  client with a `conversation.item.truncate` event. This event is used to
  synchronize the server's understanding of the audio with the client's playback.
  
  This action will truncate the audio and remove the server-side text transcript
  to ensure there is no text in the context that hasn't been heard by the user.
  """)
model RealtimeServerEventConversationItemTruncated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.truncated`.
    """)
  type: RealtimeServerEventType.conversation_item_truncated;

  /** The ID of the assistant message item that was truncated. */
  item_id: string;

  /** The index of the content part that was truncated. */
  content_index: int32;

  /** The duration up to which the audio was truncated, in milliseconds. */
  audio_end_ms: int32;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when an item in the conversation is deleted by the client with a
  `conversation.item.delete` event. This event is used to synchronize the
  server's understanding of the conversation history with the client's view.
  """)
model RealtimeServerEventConversationItemDeleted extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.deleted`.
    """)
  type: RealtimeServerEventType.conversation_item_deleted;

  /** The ID of the item that was deleted. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a new Response is created. The first event of response creation,
  where the response is in an initial state of `in_progress`.
  """)
model RealtimeServerEventResponseCreated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.created`.
    """)
  type: RealtimeServerEventType.response_created;

  response: RealtimeResponse;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a Response is done streaming. Always emitted, no matter the
  final state. The Response object included in the `response.done` event will
  include all output Items in the Response but will omit the raw audio data.
  """)
model RealtimeServerEventResponseDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.done`.
    """)
  type: RealtimeServerEventType.response_done;

  response: RealtimeResponse;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when a new Item is created during Response generation. */
model RealtimeServerEventResponseOutputItemAdded extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_item.added`.
    """)
  type: RealtimeServerEventType.response_output_item_added;

  /** The ID of the Response to which the item belongs. */
  response_id: string;

  /** The index of the output item in the Response. */
  output_index: int32;

  // Tool customization: apply enriched item definition hierarchy
  item: RealtimeConversationResponseItem;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when an Item is done streaming. Also emitted when a Response is
 * interrupted, incomplete, or cancelled.
 */
model RealtimeServerEventResponseOutputItemDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_item.done`.
    """)
  type: RealtimeServerEventType.response_output_item_done;

  /** The ID of the Response to which the item belongs. */
  response_id: string;

  /** The index of the output item in the Response. */
  output_index: int32;

  // Tool customization: apply enriched item definition hierarchy
  item: RealtimeConversationResponseItem;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when a new content part is added to an assistant message item during
 * response generation.
 */
model RealtimeServerEventResponseContentPartAdded extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.content_part.added`.
    """)
  type: RealtimeServerEventType.response_content_part_added;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item to which the content part was added. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  // Tool customization: apply detailed content part type
  /** The content part that was added. */
  part: RealtimeContentPart;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when a content part is done streaming in an assistant message item.
 * Also emitted when a Response is interrupted, incomplete, or cancelled.
 */
model RealtimeServerEventResponseContentPartDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.content_part.done`.
    """)
  type: RealtimeServerEventType.response_content_part_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  // Tool customization: apply detailed content part type
  /** The content part that is done. */
  part: RealtimeContentPart;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the text value of an "output_text" content part is updated. */
model RealtimeServerEventResponseTextDelta extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_text.delta`.
    """)
  type: RealtimeServerEventType.response_text_delta;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  /** The text delta. */
  delta: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when the text value of an "output_text" content part is done streaming. Also
 * emitted when a Response is interrupted, incomplete, or cancelled.
 */
model RealtimeServerEventResponseTextDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_text.done`.
    """)
  type: RealtimeServerEventType.response_text_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  /** The final text content. */
  text: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the model-generated transcription of audio output is updated. */
model RealtimeServerEventResponseAudioTranscriptDelta
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_audio_transcript.delta`.
    """)
  type: RealtimeServerEventType.response_audio_transcript_delta;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  /** The transcript delta. */
  delta: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when the model-generated transcription of audio output is done
 * streaming. Also emitted when a Response is interrupted, incomplete, or
 * cancelled.
 */
model RealtimeServerEventResponseAudioTranscriptDone
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_audio_transcript.done`.
    """)
  type: RealtimeServerEventType.response_audio_transcript_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  /** The final transcript of the audio. */
  transcript: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the model-generated audio is updated. */
model RealtimeServerEventResponseAudioDelta extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_audio.delta`.
    """)
  type: RealtimeServerEventType.response_audio_delta;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;

  // Tool customization: use encoded type for audio data
  /** Base64-encoded audio data delta. */
  @encode("base64")
  delta: bytes;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when the model-generated audio is done. Also emitted when a Response
 * is interrupted, incomplete, or cancelled.
 */
model RealtimeServerEventResponseAudioDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.output_audio.done`.
    """)
  type: RealtimeServerEventType.response_audio_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The index of the content part in the item's content array. */
  content_index: int32;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the model-generated function call arguments are updated. */
model RealtimeServerEventResponseFunctionCallArgumentsDelta
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.function_call_arguments.delta`.
    """)
  type: RealtimeServerEventType.response_function_call_arguments_delta;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the function call item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The ID of the function call. */
  call_id: string;

  /** The arguments delta as a JSON string. */
  delta: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Returned when the model-generated function call arguments are done streaming.
 * Also emitted when a Response is interrupted, incomplete, or cancelled.
 */
model RealtimeServerEventResponseFunctionCallArgumentsDone
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.function_call_arguments.done`.
    """)
  type: RealtimeServerEventType.response_function_call_arguments_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the function call item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The ID of the function call. */
  call_id: string;

  /** The final arguments as a JSON string. */
  arguments: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/**
 * Emitted at the beginning of a Response to indicate the updated rate limits.
 * When a Response is created some tokens will be "reserved" for the output
 * tokens, the rate limits shown here reflect that reservation, which is then
 * adjusted accordingly once the Response is completed.
 */
model RealtimeServerEventRateLimitsUpdated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `rate_limits.updated`.
    """)
  type: RealtimeServerEventType.rate_limits_updated;

  // Tool customization: use custom type for rate limit items (applying encoded duration)
  /** List of rate limit information. */
  rate_limits: RealtimeServerEventRateLimitsUpdatedRateLimitsItem[];
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  **SIP Only:** Returned when a DTMF event is received. A DTMF event is a message that
  represents a telephone keypad press (0–9, *, #, A–D). The `event` property
  is the keypad that the user pressed. The `received_at` is the UTC Unix Timestamp
  that the server received the event.
  """)
model RealtimeServerEventInputAudioBufferDtmfEventReceived
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.dtmf_event_received`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_dtmf_event_received;

  /** The telephone keypad that was pressed by the user. */
  event: string;

  /** The Unix timestamp when the DTMF event was received. */
  @encode("unixTimestamp", int32)
  received_at: utcDateTime;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when the server detects a timeout when `server_vad` mode is enabled with an
  `idle_timeout_ms` configured. This event can happen if there is no audio from the user
  for more than `idle_timeout_ms` milliseconds. This might be a good opportunity to prompt
  the user for something relevant to the conversation or a prompt to continue speaking.
  """)
model RealtimeServerEventInputAudioBufferTimeoutTriggered
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `input_audio_buffer.timeout_triggered`.
    """)
  type: RealtimeServerEventType.input_audio_buffer_timeout_triggered;

  @doc("""
    Millisecond offset of audio written to the input audio buffer that was after
    the playback time of the last model response.
    """)
  audio_start_ms: int32;

  @doc("""
    Millisecond offset of audio written to the input audio buffer at the time
    the timeout was triggered.
    """)
  audio_end_ms: int32;

  /** The ID of the user message item that will be created. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when an input audio transcription segment is identified for an item.
  This is used for diarization and segmentation of the input audio transcription.
  """)
model RealtimeServerEventConversationItemInputAudioTranscriptionSegment
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.input_audio_transcription.segment`.
    """)
  type: RealtimeServerEventType.conversation_item_input_audio_transcription_segment;

  /** The ID of the item. */
  item_id: string;

  /** The index of the content part containing the audio. */
  content_index: int32;

  /** The transcribed text for the segment. */
  text: string;

  /** The unique ID of the segment. */
  id: string;

  /** The speaker identifier for the segment. */
  speaker?: string;

  /** Start time of the segment in seconds. */
  start: float32;

  /** End time of the segment in seconds. */
  end: float32;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when listing MCP tools is in progress for an item. */
model RealtimeServerEventMCPListToolsInProgress extends RealtimeServerEvent {
  @doc("""
    The event type, must be `mcp_list_tools.in_progress`.
    """)
  type: RealtimeServerEventType.mcp_list_tools_in_progress;

  /** The ID of the MCP list tools item. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when listing MCP tools has completed for an item. */
model RealtimeServerEventMCPListToolsCompleted extends RealtimeServerEvent {
  @doc("""
    The event type, must be `mcp_list_tools.completed`.
    """)
  type: RealtimeServerEventType.mcp_list_tools_completed;

  /** The ID of the MCP list tools item. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when listing MCP tools has failed for an item. */
model RealtimeServerEventMCPListToolsFailed extends RealtimeServerEvent {
  @doc("""
    The event type, must be `mcp_list_tools.failed`.
    """)
  type: RealtimeServerEventType.mcp_list_tools_failed;

  /** The ID of the MCP list tools item. */
  item_id: string;

  /** Details of the error. */
  error: RealtimeMCPError;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the model-generated MCP tool call arguments are updated. */
model RealtimeServerEventResponseMCPCallArgumentsDelta extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.mcp_call_arguments.delta`.
    """)
  type: RealtimeServerEventType.response_mcp_call_arguments_delta;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the MCP tool call item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The arguments delta as a JSON string. */
  delta: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when MCP tool call arguments are finalized during response generation. */
model RealtimeServerEventResponseMCPCallArgumentsDone extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.mcp_call_arguments.done`.
    """)
  type: RealtimeServerEventType.response_mcp_call_arguments_done;

  /** The ID of the response. */
  response_id: string;

  /** The ID of the MCP tool call item. */
  item_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The final JSON-encoded arguments string. */
  arguments: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when an MCP tool call is in progress. */
model RealtimeServerEventResponseMCPCallInProgress extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.mcp_call.in_progress`.
    """)
  type: RealtimeServerEventType.response_mcp_call_in_progress;

  /** The ID of the response. */
  response_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The ID of the MCP tool call item. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when an MCP tool call has completed successfully. */
model RealtimeServerEventResponseMCPCallCompleted extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.mcp_call.completed`.
    """)
  type: RealtimeServerEventType.response_mcp_call_completed;

  /** The ID of the response. */
  response_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The ID of the MCP tool call item. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when an MCP tool call has failed. */
model RealtimeServerEventResponseMCPCallFailed extends RealtimeServerEvent {
  @doc("""
    The event type, must be `response.mcp_call.failed`.
    """)
  type: RealtimeServerEventType.response_mcp_call_failed;

  /** The ID of the response. */
  response_id: string;

  /** The index of the output item in the response. */
  output_index: int32;

  /** The ID of the MCP tool call item. */
  item_id: string;

  /** Details of the error. */
  error: RealtimeMCPError;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when a transcription session is created. */
model RealtimeServerEventTranscriptionSessionCreated extends RealtimeServerEvent {
  @doc("""
    The event type, must be `transcription_session.created`.
    """)
  type: RealtimeServerEventType.transcription_session_created;

  session: RealtimeTranscriptionSessionCreateResponse;
}

/** Create a new Realtime response with these parameters */
model RealtimeResponseCreateParams {
  // Tool customization: Apply reusable modality representation
  /**
   * The set of modalities the model can respond with. To disable audio,
   * set this to ["text"].
   */
  modalities?: RealtimeModality[];

  @doc("""
    The default system instructions (i.e. system message) prepended to model
    calls. This field allows the client to guide the model on desired
    responses. The model can be instructed on response content and format,
    (e.g. "be extremely succinct", "act friendly", "here are examples of good
    responses") and on audio behavior (e.g. "talk quickly", "inject emotion
    into your voice", "laugh frequently"). The instructions are not guaranteed
    to be followed by the model, but they provide guidance to the model on the
    desired behavior.
    
    Note that the server sets default instructions which will be used if this
    field is not set and are visible in the `session.created` event at the
    start of the session.
    """)
  instructions?: string;

  @doc("""
    The voice the model uses to respond. Voice cannot be changed during the
    session once the model has responded with audio at least once. Current
    voice options are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`,
    `onyx`, `nova`, `sage`, `shimmer`, and `verse`.
    """)
  voice?: VoiceIdsShared;

  // Tool customization: use extracted and reusable audio format definition
  @doc("""
    The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    """)
  output_audio_format?: RealtimeAudioFormat = RealtimeAudioFormat.pcm16;

  // Tool customization: use enriched tool definition
  /** Tools (functions) available to the model. */
  tools?: RealtimeTool[];

  @doc("""
    How the model chooses tools. Options are `auto`, `none`, `required`, or
    specify a function, like `{"type": "function", "function": {"name": "my_function"}}`.
    """)
  tool_choice?: string;

  /** Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. */
  temperature?: float32;

  // Tool customization: Address (observed as of 2025-01-31) spec issue with 'max_response_output_tokens'
  @doc("""
    Maximum number of output tokens for a single assistant response,
    inclusive of tool calls. Provide an integer between 1 and 4096 to
    limit output tokens, or `inf` for the maximum available tokens for a
    given model. Defaults to `inf`.
    """)
  max_output_tokens?: int32 | "inf";

  @doc("""
    Controls which conversation the response is added to. Currently supports
    `auto` and `none`, with `auto` as the default value. The `auto` value
    means that the contents of the response will be added to the default
    conversation. Set this to `none` to create an out-of-band response which
    will not add items to default conversation.
    """)
  conversation?: string | "auto" | "none";

  ...MetadataPropertyForRequest;

  // Tool customization: apply a customized, more specific discriminated type hierarchy
  @doc("""
    Input items to include in the prompt for the model. Using this field
    creates a new context for this Response instead of using the default
    conversation. An empty array `[]` will clear the context for this Response.
    Note that this can include references to items from the default conversation.
    """)
  input?: RealtimeConversationRequestItem[];
}

/** Realtime session object configuration. */
model RealtimeSessionCreateRequest {
  // Tool customization: Apply reusable modality representation
  /**
   * The set of modalities the model can respond with. To disable audio,
   * set this to ["text"].
   */
  modalities?: RealtimeModality[];

  /** The Realtime model used for this session. */
  `model`?:
    | string
    | "gpt-realtime"
    | "gpt-realtime-2025-08-28"
    | "gpt-4o-realtime-preview"
    | "gpt-4o-realtime-preview-2024-10-01"
    | "gpt-4o-realtime-preview-2024-12-17"
    | "gpt-4o-realtime-preview-2025-06-03"
    | "gpt-4o-mini-realtime-preview"
    | "gpt-4o-mini-realtime-preview-2024-12-17"
    | "gpt-realtime-mini"
    | "gpt-realtime-mini-2025-10-06"
    | "gpt-realtime-mini-2025-12-15"
    | "gpt-audio-mini"
    | "gpt-audio-mini-2025-10-06"
    | "gpt-audio-mini-2025-12-15";

  @doc("""
    The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. "be extremely succinct", "act friendly", "here are examples of good responses") and on audio behavior (e.g. "talk quickly", "inject emotion into your voice", "laugh frequently"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.
    
    Note that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.
    """)
  instructions?: string;

  @doc("""
    The voice the model uses to respond. Voice cannot be changed during the
    session once the model has responded with audio at least once. Current
    voice options are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`,
    `onyx`, `nova`, `sage`, `shimmer`, and `verse`.
    """)
  voice?: VoiceIdsShared;

  // Tool customization: use extracted and reusable audio format definition
  @doc("""
    The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    For `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate,
    single channel (mono), and little-endian byte order.
    """)
  input_audio_format?: RealtimeAudioFormat = RealtimeAudioFormat.pcm16;

  // Tool customization: use extracted and reusable audio format definition
  @doc("""
    The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    For `pcm16`, output audio is sampled at a rate of 24kHz.
    """)
  output_audio_format?: RealtimeAudioFormat = RealtimeAudioFormat.pcm16;

  @doc("""
    Configuration for input audio transcription, defaults to off and can be set to `null` to turn off once on. Input audio transcription is not native to the model, since the model consumes audio directly. Transcription runs asynchronously through [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription) and should be treated as guidance of input audio content rather than precisely what the model heard. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service.
    """)
  input_audio_transcription?: {
    @doc("""
      The model to use for transcription, current options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`.
      """)
    `model`?: string;

    @doc("""
      The language of the input audio. Supplying the input language in
      [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format
      will improve accuracy and latency.
      """)
    language?: string;

    @doc("""
      An optional text to guide the model's style or continue a previous audio
      segment.
      For `whisper-1`, the [prompt is a list of keywords](/docs/guides/speech-to-text#prompting).
      For `gpt-4o-transcribe` models, the prompt is a free text string, for example "expect words related to technology".
      """)
    prompt?: string;
  };

  @doc("""
    Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.
    Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.
    Semantic VAD is more advanced and uses a turn detection model (in conjuction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with "uhhm", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.
    """)
  turn_detection?: {
    /** Type of turn detection. */
    type?: "server_vad" | "semantic_vad" = "server_vad";

    @doc("""
      Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`.
      """)
    eagerness?: "low" | "medium" | "high" | "auto" = "auto";

    @doc("""
      Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
      higher threshold will require louder audio to activate the model, and
      thus might perform better in noisy environments.
      """)
    threshold?: float32;

    @doc("""
      Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in
      milliseconds). Defaults to 300ms.
      """)
    prefix_padding_ms?: int32;

    @doc("""
      Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults
      to 500ms. With shorter values the model will respond more quickly,
      but may jump in on short pauses from the user.
      """)
    silence_duration_ms?: int32;

    /** Whether or not to automatically generate a response when a VAD stop event occurs. */
    create_response?: boolean = true;

    @doc("""
      Whether or not to automatically interrupt any ongoing response with output to the default
      conversation (i.e. `conversation` of `auto`) when a VAD start event occurs.
      """)
    interrupt_response?: boolean = true;
  };

  @doc("""
    Configuration for input audio noise reduction. This can be set to `null` to turn off.
    Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
    Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
    """)
  input_audio_noise_reduction?: {
    @doc("""
      Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.
      """)
    type?: "near_field" | "far_field";
  } | null = null;

  /**
   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is
   * the minimum speed. 1.5 is the maximum speed. This value can only be changed
   * in between model turns, not while a response is in progress.
   */
  @minValue(0.25)
  @maxValue(1.5)
  speed?: float32 = 1;

  @doc("""
    Configuration options for tracing. Set to null to disable tracing. Once
    tracing is enabled for a session, the configuration cannot be modified.
    
    `auto` will create a trace for the session with default values for the
    workflow name, group id, and metadata.
    """)
  tracing?: "auto" | {
    /**
     * The name of the workflow to attach to this trace. This is used to
     * name the trace in the traces dashboard.
     */
    workflow_name?: string;

    /**
     * The group id to attach to this trace to enable filtering and
     * grouping in the traces dashboard.
     */
    group_id?: string;

    /**
     * The arbitrary metadata to attach to this trace to enable
     * filtering in the traces dashboard.
     */
    metadata?: unknown;
  };

  // Tool customization: use enriched tool definition
  /** Tools (functions) available to the model. */
  tools?: RealtimeTool[];

  @doc("""
    How the model chooses tools. Options are `auto`, `none`, `required`, or
    specify a function.
    """)
  tool_choice?: string = "auto";

  /** Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a temperature of 0.8 is highly recommended for best performance. */
  temperature?: float32 = 0.8;

  @doc("""
    Maximum number of output tokens for a single assistant response,
    inclusive of tool calls. Provide an integer between 1 and 4096 to
    limit output tokens, or `inf` for the maximum available tokens for a
    given model. Defaults to `inf`.
    """)
  max_response_output_tokens?: int32 | "inf";

  /** Configuration options for the generated client secret. */
  client_secret?: {
    /** Configuration for the ephemeral token expiration. */
    expires_at?: {
      @doc("""
        The anchor point for the ephemeral token expiration. Only `created_at` is currently supported.
        """)
      anchor?: "created_at" = "created_at";

      @doc("""
        The number of seconds from the anchor point to the expiration. Select a value between `10` and `7200`.
        """)
      seconds?: int32 = 600;
    };
  };
}

/**
 * A new Realtime session configuration, with an ephermeral key. Default TTL
 * for keys is one minute.
 */
model RealtimeSessionCreateResponse {
  /** Ephemeral key returned by the API. */
  client_secret: {
    /**
     * Ephemeral key usable in client environments to authenticate connections
     * to the Realtime API. Use this in client-side environments rather than
     * a standard API token, which should only be used server-side.
     */
    value: string;

    // Tool customization: 'created' and fields ending in '_at' are Unix encoded utcDateTime
    /**
     * Timestamp for when the token expires. Currently, all tokens expire
     * after one minute.
     */
    @encode("unixTimestamp", int32)
    expires_at: utcDateTime;
  };

  // Tool customization: Apply reusable modality representation
  /**
   * The set of modalities the model can respond with. To disable audio,
   * set this to ["text"].
   */
  modalities?: RealtimeModality[];

  @doc("""
    The default system instructions (i.e. system message) prepended to model
    calls. This field allows the client to guide the model on desired
    responses. The model can be instructed on response content and format,
    (e.g. "be extremely succinct", "act friendly", "here are examples of good
    responses") and on audio behavior (e.g. "talk quickly", "inject emotion
    into your voice", "laugh frequently"). The instructions are not guaranteed
    to be followed by the model, but they provide guidance to the model on the
    desired behavior.
    
    Note that the server sets default instructions which will be used if this
    field is not set and are visible in the `session.created` event at the
    start of the session.
    """)
  instructions?: string;

  @doc("""
    The voice the model uses to respond. Voice cannot be changed during the
    session once the model has responded with audio at least once. Current
    voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`,
    `shimmer` and `verse`.
    """)
  voice?: VoiceIdsShared;

  // Tool customization: use extracted and reusable audio format definition
  @doc("""
    The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    """)
  input_audio_format?: RealtimeAudioFormat;

  // Tool customization: use extracted and reusable audio format definition
  @doc("""
    The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    """)
  output_audio_format?: RealtimeAudioFormat;

  @doc("""
    Configuration for input audio transcription, defaults to off and can be
    set to `null` to turn off once on. Input audio transcription is not native
    to the model, since the model consumes audio directly. Transcription runs
    asynchronously through Whisper and should be treated as rough guidance
    rather than the representation understood by the model.
    """)
  input_audio_transcription?: {
    @doc("""
      The model to use for transcription, `whisper-1` is the only currently
      supported model.
      """)
    `model`?: string;
  };

  /**
   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is
   * the minimum speed. 1.5 is the maximum speed. This value can only be changed
   * in between model turns, not while a response is in progress.
   */
  @minValue(0.25)
  @maxValue(1.5)
  speed?: float32 = 1;

  @doc("""
    Configuration options for tracing. Set to null to disable tracing. Once
    tracing is enabled for a session, the configuration cannot be modified.
    
    `auto` will create a trace for the session with default values for the
    workflow name, group id, and metadata.
    """)
  tracing?: "auto" | {
    /**
     * The name of the workflow to attach to this trace. This is used to
     * name the trace in the traces dashboard.
     */
    workflow_name?: string;

    /**
     * The group id to attach to this trace to enable filtering and
     * grouping in the traces dashboard.
     */
    group_id?: string;

    /**
     * The arbitrary metadata to attach to this trace to enable
     * filtering in the traces dashboard.
     */
    metadata?: unknown;
  };

  @doc("""
    Configuration for turn detection. Can be set to `null` to turn off. Server
    VAD means that the model will detect the start and end of speech based on
    audio volume and respond at the end of user speech.
    """)
  turn_detection?: {
    @doc("""
      Type of turn detection, only `server_vad` is currently supported.
      """)
    type?: string;

    /**
     * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
     * higher threshold will require louder audio to activate the model, and
     * thus might perform better in noisy environments.
     */
    threshold?: float32;

    /**
     * Amount of audio to include before the VAD detected speech (in
     * milliseconds). Defaults to 300ms.
     */
    prefix_padding_ms?: int32;

    /**
     * Duration of silence to detect speech stop (in milliseconds). Defaults
     * to 500ms. With shorter values the model will respond more quickly,
     * but may jump in on short pauses from the user.
     */
    silence_duration_ms?: int32;
  };

  // Tool customization: use enriched tool definition
  /** Tools (functions) available to the model. */
  tools?: RealtimeTool[];

  @doc("""
    How the model chooses tools. Options are `auto`, `none`, `required`, or
    specify a function.
    """)
  tool_choice?: string;

  /** Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. */
  temperature?: float32;

  @doc("""
    Maximum number of output tokens for a single assistant response,
    inclusive of tool calls. Provide an integer between 1 and 4096 to
    limit output tokens, or `inf` for the maximum available tokens for a
    given model. Defaults to `inf`.
    """)
  max_response_output_tokens?: int32 | "inf";
}

/** The item to add to the conversation. */
model RealtimeConversationItemWithReference {
  @doc("""
    For an item of type (`message` | `function_call` | `function_call_output`)
    this field allows the client to assign the unique ID of the item. It is
    not required because the server will generate one if not provided.
    
    For an item of type `item_reference`, this field is required and is a
    reference to any item that has previously existed in the conversation.
    """)
  id?: string;

  @doc("""
    The type of the item (`message`, `function_call`, `function_call_output`, `item_reference`).
    """)
  type?: "message" | "function_call" | "function_call_output";

  @doc("""
    Identifier for the API object being returned - always `realtime.item`.
    """)
  object?: "realtime.item";

  @doc("""
    The status of the item (`completed`, `incomplete`). These have no effect
    on the conversation, but are accepted for consistency with the
    `conversation.item.created` event.
    """)
  status?: "completed" | "incomplete";

  @doc("""
    The role of the message sender (`user`, `assistant`, `system`), only
    applicable for `message` items.
    """)
  role?: "user" | "assistant" | "system";

  @doc("""
    The content of the message, applicable for `message` items.
    - Message items of role `system` support only `input_text` content
    - Message items of role `user` support `input_text` and `input_audio`
      content
    - Message items of role `assistant` support `text` content.
    """)
  content?: RealtimeConversationItemWithReferenceContent[];

  @doc("""
    The ID of the function call (for `function_call` and
    `function_call_output` items). If passed on a `function_call_output`
    item, the server will check that a `function_call` item with the same
    ID exists in the conversation history.
    """)
  call_id?: string;

  @doc("""
    The name of the function being called (for `function_call` items).
    """)
  name?: string;

  @doc("""
    The arguments of the function call (for `function_call` items).
    """)
  arguments?: string;

  @doc("""
    The output of the function call (for `function_call_output` items).
    """)
  output?: string;
}

// Tool customization (apply_discriminator): apply discriminated type base
@doc("""
  Send this event when you want to retrieve the server's representation of a specific item in the conversation history. This is useful, for example, to inspect user audio after noise cancellation and VAD.
  The server will respond with a `conversation.item.retrieved` event,
  unless the item does not exist in the conversation history, in which case the
  server will respond with an error.
  """)
model RealtimeClientEventConversationItemRetrieve extends RealtimeClientEvent {
  @doc("""
    The event type, must be `conversation.item.retrieve`.
    """)
  type: RealtimeClientEventType.conversation_item_retrieve;

  /** The ID of the item to retrieve. */
  item_id: string;
}

// Tool customization (apply_discriminator): apply discriminated type base
/** Send this event to update a transcription session. */
model RealtimeClientEventTranscriptionSessionUpdate
  extends RealtimeClientEvent {
  @doc("""
    The event type, must be `transcription_session.update`.
    """)
  type: RealtimeClientEventType.transcription_session_update;

  session: RealtimeTranscriptionSessionCreateRequest;
}

// Tool customization (apply_discriminator): apply discriminated type
/** Returned when the text value of an input audio transcription content part is updated. */
model RealtimeServerEventConversationItemInputAudioTranscriptionDelta
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.input_audio_transcription.delta`.
    """)
  type: RealtimeServerEventType.conversation_item_input_audio_transcription_delta;

  /** The ID of the item. */
  item_id: string;

  /** The index of the content part in the item's content array. */
  content_index?: int32;

  /** The text delta. */
  delta?: string;

  /** The log probabilities of the transcription. */
  logprobs?: LogProbProperties[] | null;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a conversation item is retrieved with `conversation.item.retrieve`.
  """)
model RealtimeServerEventConversationItemRetrieved extends RealtimeServerEvent {
  @doc("""
    The event type, must be `conversation.item.retrieved`.
    """)
  type: RealtimeServerEventType.conversation_item_retrieved;

  // Tool customization: apply enriched item definition hierarchy
  item: RealtimeConversationResponseItem;
}

// Tool customization (apply_discriminator): apply discriminated type
@doc("""
  Returned when a transcription session is updated with a `transcription_session.update` event, unless
  there is an error.
  """)
model RealtimeServerEventTranscriptionSessionUpdated
  extends RealtimeServerEvent {
  @doc("""
    The event type, must be `transcription_session.updated`.
    """)
  type: RealtimeServerEventType.transcription_session_updated;

  session: RealtimeTranscriptionSessionCreateResponse;
}

/** Realtime transcription session object configuration. */
model RealtimeTranscriptionSessionCreateRequest {
  /**
   * The set of modalities the model can respond with. To disable audio,
   * set this to ["text"].
   */
  modalities?: ("text" | "audio")[];

  @doc("""
    The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    For `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate,
    single channel (mono), and little-endian byte order.
    """)
  input_audio_format?: "pcm16" | "g711_ulaw" | "g711_alaw" = "pcm16";

  /** Configuration for input audio transcription. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service. */
  input_audio_transcription?: {
    @doc("""
      The model to use for transcription, current options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`.
      """)
    `model`?: "gpt-4o-transcribe" | "gpt-4o-mini-transcribe" | "whisper-1";

    @doc("""
      The language of the input audio. Supplying the input language in
      [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format
      will improve accuracy and latency.
      """)
    language?: string;

    @doc("""
      An optional text to guide the model's style or continue a previous audio
      segment.
      For `whisper-1`, the [prompt is a list of keywords](/docs/guides/speech-to-text#prompting).
      For `gpt-4o-transcribe` models, the prompt is a free text string, for example "expect words related to technology".
      """)
    prompt?: string;
  };

  @doc("""
    Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.
    Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.
    Semantic VAD is more advanced and uses a turn detection model (in conjuction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with "uhhm", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.
    """)
  turn_detection?: {
    /** Type of turn detection. */
    type?: "server_vad" | "semantic_vad" = "server_vad";

    @doc("""
      Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`.
      """)
    eagerness?: "low" | "medium" | "high" | "auto" = "auto";

    @doc("""
      Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
      higher threshold will require louder audio to activate the model, and
      thus might perform better in noisy environments.
      """)
    threshold?: float32;

    @doc("""
      Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in
      milliseconds). Defaults to 300ms.
      """)
    prefix_padding_ms?: int32;

    @doc("""
      Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults
      to 500ms. With shorter values the model will respond more quickly,
      but may jump in on short pauses from the user.
      """)
    silence_duration_ms?: int32;

    /** Whether or not to automatically generate a response when a VAD stop event occurs. Not available for transcription sessions. */
    create_response?: boolean = true;

    @doc("""
      Whether or not to automatically interrupt any ongoing response with output to the default
      conversation (i.e. `conversation` of `auto`) when a VAD start event occurs. Not available for transcription sessions.
      """)
    interrupt_response?: boolean = true;
  };

  @doc("""
    Configuration for input audio noise reduction. This can be set to `null` to turn off.
    Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
    Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
    """)
  input_audio_noise_reduction?: {
    @doc("""
      Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.
      """)
    type?: "near_field" | "far_field";
  } | null = null;

  @doc("""
    The set of items to include in the transcription. Current available items are:
    - `item.input_audio_transcription.logprobs`
    """)
  include?: string[];

  /** Configuration options for the generated client secret. */
  client_secret?: {
    /** Configuration for the ephemeral token expiration. */
    expires_at?: {
      @doc("""
        The anchor point for the ephemeral token expiration. Only `created_at` is currently supported.
        """)
      anchor?: "created_at" = "created_at";

      @doc("""
        The number of seconds from the anchor point to the expiration. Select a value between `10` and `7200`.
        """)
      seconds?: int32 = 600;
    };
  };
}

/**
 * A new Realtime transcription session configuration.
 *
 * When a session is created on the server via REST API, the session object
 * also contains an ephemeral key. Default TTL for keys is 10 minutes. This
 * property is not present when a session is updated via the WebSocket API.
 */
model RealtimeTranscriptionSessionCreateResponse {
  /**
   * Ephemeral key returned by the API. Only present when the session is
   * created on the server via REST API.
   */
  client_secret: {
    /**
     * Ephemeral key usable in client environments to authenticate connections
     * to the Realtime API. Use this in client-side environments rather than
     * a standard API token, which should only be used server-side.
     */
    value: string;

    // Tool customization: 'created' and fields ending in '_at' are Unix encoded utcDateTime
    /**
     * Timestamp for when the token expires. Currently, all tokens expire
     * after one minute.
     */
    @encode("unixTimestamp", int32)
    expires_at: utcDateTime;
  };

  /**
   * The set of modalities the model can respond with. To disable audio,
   * set this to ["text"].
   */
  modalities?: ("text" | "audio")[];

  @doc("""
    The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
    """)
  input_audio_format?: string;

  /** Configuration of the transcription model. */
  input_audio_transcription?: {
    @doc("""
      The model to use for transcription. Can be `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, or `whisper-1`.
      """)
    `model`?: "gpt-4o-transcribe" | "gpt-4o-mini-transcribe" | "whisper-1";

    @doc("""
      The language of the input audio. Supplying the input language in
      [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format
      will improve accuracy and latency.
      """)
    language?: string;

    /**
     * An optional text to guide the model's style or continue a previous audio
     * segment. The [prompt](/docs/guides/speech-to-text#prompting) should match
     * the audio language.
     */
    prompt?: string;
  };

  @doc("""
    Configuration for turn detection. Can be set to `null` to turn off. Server
    VAD means that the model will detect the start and end of speech based on
    audio volume and respond at the end of user speech.
    """)
  turn_detection?: {
    @doc("""
      Type of turn detection, only `server_vad` is currently supported.
      """)
    type?: string;

    /**
     * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
     * higher threshold will require louder audio to activate the model, and
     * thus might perform better in noisy environments.
     */
    threshold?: float32;

    /**
     * Amount of audio to include before the VAD detected speech (in
     * milliseconds). Defaults to 300ms.
     */
    prefix_padding_ms?: int32;

    /**
     * Duration of silence to detect speech stop (in milliseconds). Defaults
     * to 500ms. With shorter values the model will respond more quickly,
     * but may jump in on short pauses from the user.
     */
    silence_duration_ms?: int32;
  };
}

/**
 * The anchor point from which client secret expiration is calculated.
 */
union RealtimeCreateClientSecretRequestExpiresAfterAnchor {
  string,
  /** Expiration is calculated from the time the client secret is created. */
  created_at: "created_at",
}

/**
 * Configuration for the client secret expiration.
 */
model RealtimeCreateClientSecretRequestExpiresAfter {
  /**
   * The anchor point from which the expiration is calculated.
   */
  anchor?: RealtimeCreateClientSecretRequestExpiresAfterAnchor;

  /**
   * The number of seconds after the anchor point when the client secret expires.
   */
  seconds?: integer;
}

/**
 * The type of session to create.
 */
union RealtimeSessionCreateRequestUnionType {
  string,
  /** A real-time conversation session. */
  realtime: "realtime",
  /** A transcription session. */
  transcription: "transcription",
}

/**
 * Base model for a session creation request.
 */
@discriminator("type")
model RealtimeSessionCreateRequestUnion {
  /** The type of session to create. */
  type: RealtimeSessionCreateRequestUnionType;
}

/**
 * Request to create a session with a realtime configuration.
 */
model RealtimeSessionCreateRealtimeRequest extends RealtimeSessionCreateRequestUnion {
  /** The type of session. */
  type: "realtime";

  /** The realtime session configuration. */
  session?: RealtimeRequestSession;
}

/**
 * Request to create a session with a transcription configuration.
 */
model RealtimeSessionCreateTranscriptionRequest extends RealtimeSessionCreateRequestUnion {
  /** The type of session. */
  type: "transcription";

  /** The transcription session configuration. */
  session?: RealtimeTranscriptionSessionCreateRequest;
}

/**
 * The type of session that was created.
 */
union RealtimeSessionCreateResponseUnionType {
  string,
  /** A real-time conversation session. */
  realtime: "realtime",
  /** A transcription session. */
  transcription: "transcription",
}

/**
 * Base model for a session creation response.
 */
@discriminator("type")
model RealtimeSessionCreateResponseUnion {
  /** The type of session that was created. */
  type: RealtimeSessionCreateResponseUnionType;
}

/**
 * Response for a realtime session.
 */
model RealtimeSessionCreateRealtimeResponse extends RealtimeSessionCreateResponseUnion {
  /** The type of session. */
  type: "realtime";

  /** The realtime session details. */
  session?: RealtimeResponseSession;
}

/**
 * Response for a transcription session.
 */
model RealtimeSessionCreateTranscriptionResponse extends RealtimeSessionCreateResponseUnion {
  /** The type of session. */
  type: "transcription";

  /** The transcription session details. */
  session?: RealtimeTranscriptionSessionCreateResponse;
}

/**
 * The type of session that was updated.
 */
union RealtimeSessionUpdatedResponseUnionType {
  string,
  /** A real-time conversation session. */
  realtime: "realtime",
  /** A transcription session. */
  transcription: "transcription",
}

/**
 * Base model for session updated response.
 */
@discriminator("type")
model RealtimeSessionUpdatedResponseUnion {
  /** The type of session that was updated. */
  type: RealtimeSessionUpdatedResponseUnionType;
}

/**
 * Session updated response for a realtime session.
 */
model RealtimeSessionUpdatedRealtimeResponse extends RealtimeSessionUpdatedResponseUnion {
  /** The type of session. */
  type: "realtime";

  ...RealtimeResponseSession;
}

/**
 * Session updated response for a transcription session.
 */
model RealtimeSessionUpdatedTranscriptionResponse extends RealtimeSessionUpdatedResponseUnion {
  /** The type of session. */
  type: "transcription";

  ...RealtimeTranscriptionSessionCreateResponse;
}

/**
 * Request to create a Realtime client secret with an associated session configuration.
 * The request can specify either a realtime or a transcription session configuration.
 */
@summary("Realtime client secret creation request")
model RealtimeCreateClientSecretRequest {
  @doc("""
    Configuration for the client secret expiration. Expiration refers to the time after which
    a client secret will no longer be valid for creating sessions. The session itself may
    continue after that time once started. A secret can be used to create multiple sessions
    until it expires.
    """)
  @summary("Client secret expiration")
  expires_after?: RealtimeCreateClientSecretRequestExpiresAfter;

  /**
   * Session configuration to use for the client secret. Choose either a realtime
   * session or a transcription session.
   */
  session?: RealtimeSessionCreateRequestUnion;
}

/**
 * Response containing the Realtime client secret and session details.
 */
model RealtimeCreateClientSecretResponse {
  /** The generated client secret value. */
  value: string;

  /** Expiration timestamp for the client secret, in seconds since epoch. */
  @encode("unixTimestamp", int64)
  expires_at: utcDateTime;

  /** The session configuration for either a realtime or transcription session. */
  session: RealtimeSessionCreateResponseUnion;
}
